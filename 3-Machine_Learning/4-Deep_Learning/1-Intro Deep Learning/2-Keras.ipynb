{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5019607843137255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "255/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13066062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# Capa entrada\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 300,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 100,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Capa salida\n",
    "model.add(keras.layers.Dense(units = 10,\n",
    "                            activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(units = 300, activation='relu'),\n",
    "    keras.layers.Dense(units = 100, activation='relu'),\n",
    "    keras.layers.Dense(units = 10, activation='softmax')\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.flatten.Flatten object at 0x00000185C3423FC8>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05010254, -0.01068587,  0.02828723, ..., -0.06792188,\n",
       "        -0.01594454,  0.0611342 ],\n",
       "       [ 0.00970887, -0.07249174, -0.02669676, ..., -0.00335648,\n",
       "        -0.0274398 ,  0.04416718],\n",
       "       [ 0.07255368,  0.06485075,  0.06431442, ...,  0.00198733,\n",
       "        -0.05851821,  0.04748998],\n",
       "       ...,\n",
       "       [-0.03390225,  0.02782699,  0.0020127 , ...,  0.02175307,\n",
       "         0.00393757,  0.04037397],\n",
       "       [ 0.04120754,  0.01092095, -0.03232834, ...,  0.018186  ,\n",
       "        -0.03287358, -0.00296648],\n",
       "       [-0.00423901,  0.05735701,  0.05292946, ..., -0.06527325,\n",
       "        -0.04234347,  0.06331092]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.SGD(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente\n",
    "model.compile(\n",
    "    optimizer = \"sgd\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784 * 300 + 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235500\n"
     ]
    }
   ],
   "source": [
    "# 1º neurona de la 1º hidden layer\n",
    "# y = a + w1*x1 + w2*x2 + .... wn*xn\n",
    "# a es el intercepto llamado bias\n",
    "# wn es cada uno de los pesos que va a ir actualizando con el backpropagation\n",
    "# n es 784\n",
    "# En la 1º hidden layer tenemos 784 pesos por cada neurona, al tener 300, tenemos un total de:\n",
    "print(784*300 + 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 * 784 + 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 * 100 + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * 10 + 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 28, 28)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.2883 - accuracy: 0.6843 - val_loss: 0.6085 - val_accuracy: 0.8626\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.8694 - val_loss: 0.3933 - val_accuracy: 0.8995\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3992 - accuracy: 0.8917 - val_loss: 0.3323 - val_accuracy: 0.9101\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3512 - accuracy: 0.9034 - val_loss: 0.3026 - val_accuracy: 0.9172\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3224 - accuracy: 0.9094 - val_loss: 0.2833 - val_accuracy: 0.9209\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3015 - accuracy: 0.9154 - val_loss: 0.2668 - val_accuracy: 0.9247\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2849 - accuracy: 0.9199 - val_loss: 0.2553 - val_accuracy: 0.9280\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2706 - accuracy: 0.9247 - val_loss: 0.2444 - val_accuracy: 0.9317\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2583 - accuracy: 0.9276 - val_loss: 0.2348 - val_accuracy: 0.9329\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2473 - accuracy: 0.9306 - val_loss: 0.2260 - val_accuracy: 0.9360\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2370 - accuracy: 0.9332 - val_loss: 0.2204 - val_accuracy: 0.9384\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2279 - accuracy: 0.9361 - val_loss: 0.2113 - val_accuracy: 0.9409\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2197 - accuracy: 0.9383 - val_loss: 0.2054 - val_accuracy: 0.9419\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2115 - accuracy: 0.9402 - val_loss: 0.1986 - val_accuracy: 0.9463\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2044 - accuracy: 0.9428 - val_loss: 0.1932 - val_accuracy: 0.9479\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1977 - accuracy: 0.9441 - val_loss: 0.1859 - val_accuracy: 0.9500\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1911 - accuracy: 0.9460 - val_loss: 0.1807 - val_accuracy: 0.9519\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1850 - accuracy: 0.9479 - val_loss: 0.1768 - val_accuracy: 0.9527\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1792 - accuracy: 0.9494 - val_loss: 0.1718 - val_accuracy: 0.9537\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1735 - accuracy: 0.9510 - val_loss: 0.1675 - val_accuracy: 0.9542\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1685 - accuracy: 0.9525 - val_loss: 0.1653 - val_accuracy: 0.9548\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1636 - accuracy: 0.9536 - val_loss: 0.1601 - val_accuracy: 0.9567\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1590 - accuracy: 0.9552 - val_loss: 0.1567 - val_accuracy: 0.9576\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1546 - accuracy: 0.9564 - val_loss: 0.1535 - val_accuracy: 0.9597\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1504 - accuracy: 0.9577 - val_loss: 0.1495 - val_accuracy: 0.9590\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1464 - accuracy: 0.9591 - val_loss: 0.1472 - val_accuracy: 0.9599\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1426 - accuracy: 0.9596 - val_loss: 0.1434 - val_accuracy: 0.9616\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1387 - accuracy: 0.9616 - val_loss: 0.1404 - val_accuracy: 0.9621\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1353 - accuracy: 0.9618 - val_loss: 0.1391 - val_accuracy: 0.9630\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1318 - accuracy: 0.9630 - val_loss: 0.1354 - val_accuracy: 0.9628\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1287 - accuracy: 0.9642 - val_loss: 0.1336 - val_accuracy: 0.9638\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1257 - accuracy: 0.9651 - val_loss: 0.1313 - val_accuracy: 0.9645\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1226 - accuracy: 0.9657 - val_loss: 0.1294 - val_accuracy: 0.9649\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1196 - accuracy: 0.9671 - val_loss: 0.1272 - val_accuracy: 0.9657\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1170 - accuracy: 0.9677 - val_loss: 0.1267 - val_accuracy: 0.9660\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1144 - accuracy: 0.9683 - val_loss: 0.1231 - val_accuracy: 0.9666\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1117 - accuracy: 0.9695 - val_loss: 0.1226 - val_accuracy: 0.9663\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1092 - accuracy: 0.9700 - val_loss: 0.1212 - val_accuracy: 0.9670\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1070 - accuracy: 0.9702 - val_loss: 0.1183 - val_accuracy: 0.9680\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1046 - accuracy: 0.9711 - val_loss: 0.1177 - val_accuracy: 0.9678\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1024 - accuracy: 0.9720 - val_loss: 0.1149 - val_accuracy: 0.9688\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1001 - accuracy: 0.9722 - val_loss: 0.1134 - val_accuracy: 0.9691\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0981 - accuracy: 0.9733 - val_loss: 0.1120 - val_accuracy: 0.9690\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0960 - accuracy: 0.9737 - val_loss: 0.1123 - val_accuracy: 0.9688\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0940 - accuracy: 0.9742 - val_loss: 0.1095 - val_accuracy: 0.9693\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0922 - accuracy: 0.9746 - val_loss: 0.1092 - val_accuracy: 0.9706\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0904 - accuracy: 0.9754 - val_loss: 0.1070 - val_accuracy: 0.9704\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0886 - accuracy: 0.9758 - val_loss: 0.1066 - val_accuracy: 0.9702\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0866 - accuracy: 0.9762 - val_loss: 0.1057 - val_accuracy: 0.9704\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0851 - accuracy: 0.9769 - val_loss: 0.1045 - val_accuracy: 0.9709\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 50,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0844 - accuracy: 0.9768 - val_loss: 0.1050 - val_accuracy: 0.9705\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0815 - accuracy: 0.9773 - val_loss: 0.1018 - val_accuracy: 0.9720\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9785 - val_loss: 0.0990 - val_accuracy: 0.9727\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0755 - accuracy: 0.9794 - val_loss: 0.0972 - val_accuracy: 0.9729\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9806 - val_loss: 0.0971 - val_accuracy: 0.9726\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0703 - accuracy: 0.9812 - val_loss: 0.0940 - val_accuracy: 0.9739\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0677 - accuracy: 0.9818 - val_loss: 0.0949 - val_accuracy: 0.9739\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0654 - accuracy: 0.9829 - val_loss: 0.0925 - val_accuracy: 0.9737\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0633 - accuracy: 0.9831 - val_loss: 0.0900 - val_accuracy: 0.9743\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9839 - val_loss: 0.0899 - val_accuracy: 0.9739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x185c1aa6388>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 10,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [1.2883081436157227, 0.5198684930801392, 0.3992367684841156, 0.35116276144981384, 0.3224305212497711, 0.3014552593231201, 0.28485628962516785, 0.27063828706741333, 0.2583021819591522, 0.24726399779319763, 0.23700830340385437, 0.22793470323085785, 0.2197207659482956, 0.21146424114704132, 0.2044181078672409, 0.19770146906375885, 0.19108964502811432, 0.18497119843959808, 0.17924843728542328, 0.17353783547878265, 0.16847240924835205, 0.16364341974258423, 0.1590096354484558, 0.1546274572610855, 0.15043166279792786, 0.14639559388160706, 0.14261242747306824, 0.13869883120059967, 0.13531002402305603, 0.13184814155101776, 0.12866666913032532, 0.12566743791103363, 0.12262481451034546, 0.11962064355611801, 0.11697051674127579, 0.11442211270332336, 0.11166426539421082, 0.10916461795568466, 0.10697532445192337, 0.10456623882055283, 0.10240914672613144, 0.10010309517383575, 0.09806167334318161, 0.0959843099117279, 0.09403691440820694, 0.09223857522010803, 0.09036707133054733, 0.08861043304204941, 0.08664274215698242, 0.08507343381643295], 'accuracy': [0.6843400001525879, 0.8694000244140625, 0.8916800022125244, 0.903439998626709, 0.9093999862670898, 0.915440022945404, 0.9199399948120117, 0.9246799945831299, 0.9275799989700317, 0.930620014667511, 0.9332399964332581, 0.9361400008201599, 0.9383400082588196, 0.9401999711990356, 0.9427599906921387, 0.944100022315979, 0.9460399746894836, 0.947920024394989, 0.9493799805641174, 0.9509999752044678, 0.9524999856948853, 0.9535599946975708, 0.9552000164985657, 0.9564399719238281, 0.9577400088310242, 0.9591000080108643, 0.9595999717712402, 0.9615799784660339, 0.9617599844932556, 0.9629600048065186, 0.9642400145530701, 0.9650800228118896, 0.9656800031661987, 0.9670799970626831, 0.9676799774169922, 0.9682599902153015, 0.9695199728012085, 0.9699599742889404, 0.9702399969100952, 0.9710999727249146, 0.9719799757003784, 0.9721999764442444, 0.9733399748802185, 0.9736599922180176, 0.9741799831390381, 0.974560022354126, 0.9753999710083008, 0.975820004940033, 0.9762200117111206, 0.976859986782074], 'val_loss': [0.6084977388381958, 0.39331790804862976, 0.33229002356529236, 0.3025680184364319, 0.2833370566368103, 0.26679903268814087, 0.25529682636260986, 0.24444475769996643, 0.23484893143177032, 0.22603444755077362, 0.22039279341697693, 0.21127498149871826, 0.20540758967399597, 0.1985735148191452, 0.19318453967571259, 0.18594788014888763, 0.18074876070022583, 0.17680808901786804, 0.17179448902606964, 0.1674930602312088, 0.16528545320034027, 0.16012616455554962, 0.15672606229782104, 0.1534835398197174, 0.1494726985692978, 0.14717449247837067, 0.14338602125644684, 0.1403992921113968, 0.1390509307384491, 0.13537640869617462, 0.13357946276664734, 0.13128237426280975, 0.12944309413433075, 0.12719210982322693, 0.12667539715766907, 0.12310649454593658, 0.12259256839752197, 0.12119404226541519, 0.11827275156974792, 0.11765652149915695, 0.11492398381233215, 0.11343813687562943, 0.11195545643568039, 0.11234898865222931, 0.10954991728067398, 0.1091778576374054, 0.10704351961612701, 0.1065858006477356, 0.10565680265426636, 0.10446806997060776], 'val_accuracy': [0.8626000285148621, 0.8995000123977661, 0.910099983215332, 0.9172000288963318, 0.9208999872207642, 0.9247000217437744, 0.9279999732971191, 0.9316999912261963, 0.9329000115394592, 0.9359999895095825, 0.9383999705314636, 0.9409000277519226, 0.9419000148773193, 0.9463000297546387, 0.9478999972343445, 0.949999988079071, 0.9519000053405762, 0.9527000188827515, 0.9537000060081482, 0.954200029373169, 0.954800009727478, 0.9567000269889832, 0.9575999975204468, 0.9596999883651733, 0.9589999914169312, 0.9599000215530396, 0.9616000056266785, 0.9621000289916992, 0.9629999995231628, 0.9628000259399414, 0.9638000130653381, 0.9645000100135803, 0.964900016784668, 0.9656999707221985, 0.9660000205039978, 0.9666000008583069, 0.9663000106811523, 0.9670000076293945, 0.9679999947547913, 0.9678000211715698, 0.9688000082969666, 0.9690999984741211, 0.968999981880188, 0.9688000082969666, 0.9692999720573425, 0.9706000089645386, 0.9703999757766724, 0.9702000021934509, 0.9703999757766724, 0.9708999991416931]}\n"
     ]
    }
   ],
   "source": [
    "# print(history.params)\n",
    "# print(history.epoch)\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.2883081436157227,\n",
       "  0.5198684930801392,\n",
       "  0.3992367684841156,\n",
       "  0.35116276144981384,\n",
       "  0.3224305212497711,\n",
       "  0.3014552593231201,\n",
       "  0.28485628962516785,\n",
       "  0.27063828706741333,\n",
       "  0.2583021819591522,\n",
       "  0.24726399779319763,\n",
       "  0.23700830340385437,\n",
       "  0.22793470323085785,\n",
       "  0.2197207659482956,\n",
       "  0.21146424114704132,\n",
       "  0.2044181078672409,\n",
       "  0.19770146906375885,\n",
       "  0.19108964502811432,\n",
       "  0.18497119843959808,\n",
       "  0.17924843728542328,\n",
       "  0.17353783547878265,\n",
       "  0.16847240924835205,\n",
       "  0.16364341974258423,\n",
       "  0.1590096354484558,\n",
       "  0.1546274572610855,\n",
       "  0.15043166279792786,\n",
       "  0.14639559388160706,\n",
       "  0.14261242747306824,\n",
       "  0.13869883120059967,\n",
       "  0.13531002402305603,\n",
       "  0.13184814155101776,\n",
       "  0.12866666913032532,\n",
       "  0.12566743791103363,\n",
       "  0.12262481451034546,\n",
       "  0.11962064355611801,\n",
       "  0.11697051674127579,\n",
       "  0.11442211270332336,\n",
       "  0.11166426539421082,\n",
       "  0.10916461795568466,\n",
       "  0.10697532445192337,\n",
       "  0.10456623882055283,\n",
       "  0.10240914672613144,\n",
       "  0.10010309517383575,\n",
       "  0.09806167334318161,\n",
       "  0.0959843099117279,\n",
       "  0.09403691440820694,\n",
       "  0.09223857522010803,\n",
       "  0.09036707133054733,\n",
       "  0.08861043304204941,\n",
       "  0.08664274215698242,\n",
       "  0.08507343381643295],\n",
       " 'accuracy': [0.6843400001525879,\n",
       "  0.8694000244140625,\n",
       "  0.8916800022125244,\n",
       "  0.903439998626709,\n",
       "  0.9093999862670898,\n",
       "  0.915440022945404,\n",
       "  0.9199399948120117,\n",
       "  0.9246799945831299,\n",
       "  0.9275799989700317,\n",
       "  0.930620014667511,\n",
       "  0.9332399964332581,\n",
       "  0.9361400008201599,\n",
       "  0.9383400082588196,\n",
       "  0.9401999711990356,\n",
       "  0.9427599906921387,\n",
       "  0.944100022315979,\n",
       "  0.9460399746894836,\n",
       "  0.947920024394989,\n",
       "  0.9493799805641174,\n",
       "  0.9509999752044678,\n",
       "  0.9524999856948853,\n",
       "  0.9535599946975708,\n",
       "  0.9552000164985657,\n",
       "  0.9564399719238281,\n",
       "  0.9577400088310242,\n",
       "  0.9591000080108643,\n",
       "  0.9595999717712402,\n",
       "  0.9615799784660339,\n",
       "  0.9617599844932556,\n",
       "  0.9629600048065186,\n",
       "  0.9642400145530701,\n",
       "  0.9650800228118896,\n",
       "  0.9656800031661987,\n",
       "  0.9670799970626831,\n",
       "  0.9676799774169922,\n",
       "  0.9682599902153015,\n",
       "  0.9695199728012085,\n",
       "  0.9699599742889404,\n",
       "  0.9702399969100952,\n",
       "  0.9710999727249146,\n",
       "  0.9719799757003784,\n",
       "  0.9721999764442444,\n",
       "  0.9733399748802185,\n",
       "  0.9736599922180176,\n",
       "  0.9741799831390381,\n",
       "  0.974560022354126,\n",
       "  0.9753999710083008,\n",
       "  0.975820004940033,\n",
       "  0.9762200117111206,\n",
       "  0.976859986782074],\n",
       " 'val_loss': [0.6084977388381958,\n",
       "  0.39331790804862976,\n",
       "  0.33229002356529236,\n",
       "  0.3025680184364319,\n",
       "  0.2833370566368103,\n",
       "  0.26679903268814087,\n",
       "  0.25529682636260986,\n",
       "  0.24444475769996643,\n",
       "  0.23484893143177032,\n",
       "  0.22603444755077362,\n",
       "  0.22039279341697693,\n",
       "  0.21127498149871826,\n",
       "  0.20540758967399597,\n",
       "  0.1985735148191452,\n",
       "  0.19318453967571259,\n",
       "  0.18594788014888763,\n",
       "  0.18074876070022583,\n",
       "  0.17680808901786804,\n",
       "  0.17179448902606964,\n",
       "  0.1674930602312088,\n",
       "  0.16528545320034027,\n",
       "  0.16012616455554962,\n",
       "  0.15672606229782104,\n",
       "  0.1534835398197174,\n",
       "  0.1494726985692978,\n",
       "  0.14717449247837067,\n",
       "  0.14338602125644684,\n",
       "  0.1403992921113968,\n",
       "  0.1390509307384491,\n",
       "  0.13537640869617462,\n",
       "  0.13357946276664734,\n",
       "  0.13128237426280975,\n",
       "  0.12944309413433075,\n",
       "  0.12719210982322693,\n",
       "  0.12667539715766907,\n",
       "  0.12310649454593658,\n",
       "  0.12259256839752197,\n",
       "  0.12119404226541519,\n",
       "  0.11827275156974792,\n",
       "  0.11765652149915695,\n",
       "  0.11492398381233215,\n",
       "  0.11343813687562943,\n",
       "  0.11195545643568039,\n",
       "  0.11234898865222931,\n",
       "  0.10954991728067398,\n",
       "  0.1091778576374054,\n",
       "  0.10704351961612701,\n",
       "  0.1065858006477356,\n",
       "  0.10565680265426636,\n",
       "  0.10446806997060776],\n",
       " 'val_accuracy': [0.8626000285148621,\n",
       "  0.8995000123977661,\n",
       "  0.910099983215332,\n",
       "  0.9172000288963318,\n",
       "  0.9208999872207642,\n",
       "  0.9247000217437744,\n",
       "  0.9279999732971191,\n",
       "  0.9316999912261963,\n",
       "  0.9329000115394592,\n",
       "  0.9359999895095825,\n",
       "  0.9383999705314636,\n",
       "  0.9409000277519226,\n",
       "  0.9419000148773193,\n",
       "  0.9463000297546387,\n",
       "  0.9478999972343445,\n",
       "  0.949999988079071,\n",
       "  0.9519000053405762,\n",
       "  0.9527000188827515,\n",
       "  0.9537000060081482,\n",
       "  0.954200029373169,\n",
       "  0.954800009727478,\n",
       "  0.9567000269889832,\n",
       "  0.9575999975204468,\n",
       "  0.9596999883651733,\n",
       "  0.9589999914169312,\n",
       "  0.9599000215530396,\n",
       "  0.9616000056266785,\n",
       "  0.9621000289916992,\n",
       "  0.9629999995231628,\n",
       "  0.9628000259399414,\n",
       "  0.9638000130653381,\n",
       "  0.9645000100135803,\n",
       "  0.964900016784668,\n",
       "  0.9656999707221985,\n",
       "  0.9660000205039978,\n",
       "  0.9666000008583069,\n",
       "  0.9663000106811523,\n",
       "  0.9670000076293945,\n",
       "  0.9679999947547913,\n",
       "  0.9678000211715698,\n",
       "  0.9688000082969666,\n",
       "  0.9690999984741211,\n",
       "  0.968999981880188,\n",
       "  0.9688000082969666,\n",
       "  0.9692999720573425,\n",
       "  0.9706000089645386,\n",
       "  0.9703999757766724,\n",
       "  0.9702000021934509,\n",
       "  0.9703999757766724,\n",
       "  0.9708999991416931]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.288308</td>\n",
       "      <td>0.68434</td>\n",
       "      <td>0.608498</td>\n",
       "      <td>0.8626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.519868</td>\n",
       "      <td>0.86940</td>\n",
       "      <td>0.393318</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.399237</td>\n",
       "      <td>0.89168</td>\n",
       "      <td>0.332290</td>\n",
       "      <td>0.9101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.351163</td>\n",
       "      <td>0.90344</td>\n",
       "      <td>0.302568</td>\n",
       "      <td>0.9172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322431</td>\n",
       "      <td>0.90940</td>\n",
       "      <td>0.283337</td>\n",
       "      <td>0.9209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.301455</td>\n",
       "      <td>0.91544</td>\n",
       "      <td>0.266799</td>\n",
       "      <td>0.9247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.284856</td>\n",
       "      <td>0.91994</td>\n",
       "      <td>0.255297</td>\n",
       "      <td>0.9280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.270638</td>\n",
       "      <td>0.92468</td>\n",
       "      <td>0.244445</td>\n",
       "      <td>0.9317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.258302</td>\n",
       "      <td>0.92758</td>\n",
       "      <td>0.234849</td>\n",
       "      <td>0.9329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.247264</td>\n",
       "      <td>0.93062</td>\n",
       "      <td>0.226034</td>\n",
       "      <td>0.9360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.237008</td>\n",
       "      <td>0.93324</td>\n",
       "      <td>0.220393</td>\n",
       "      <td>0.9384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.227935</td>\n",
       "      <td>0.93614</td>\n",
       "      <td>0.211275</td>\n",
       "      <td>0.9409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.219721</td>\n",
       "      <td>0.93834</td>\n",
       "      <td>0.205408</td>\n",
       "      <td>0.9419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.211464</td>\n",
       "      <td>0.94020</td>\n",
       "      <td>0.198574</td>\n",
       "      <td>0.9463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.204418</td>\n",
       "      <td>0.94276</td>\n",
       "      <td>0.193185</td>\n",
       "      <td>0.9479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.197701</td>\n",
       "      <td>0.94410</td>\n",
       "      <td>0.185948</td>\n",
       "      <td>0.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.191090</td>\n",
       "      <td>0.94604</td>\n",
       "      <td>0.180749</td>\n",
       "      <td>0.9519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.94792</td>\n",
       "      <td>0.176808</td>\n",
       "      <td>0.9527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.179248</td>\n",
       "      <td>0.94938</td>\n",
       "      <td>0.171794</td>\n",
       "      <td>0.9537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.173538</td>\n",
       "      <td>0.95100</td>\n",
       "      <td>0.167493</td>\n",
       "      <td>0.9542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.168472</td>\n",
       "      <td>0.95250</td>\n",
       "      <td>0.165285</td>\n",
       "      <td>0.9548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.163643</td>\n",
       "      <td>0.95356</td>\n",
       "      <td>0.160126</td>\n",
       "      <td>0.9567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.159010</td>\n",
       "      <td>0.95520</td>\n",
       "      <td>0.156726</td>\n",
       "      <td>0.9576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.154627</td>\n",
       "      <td>0.95644</td>\n",
       "      <td>0.153484</td>\n",
       "      <td>0.9597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.150432</td>\n",
       "      <td>0.95774</td>\n",
       "      <td>0.149473</td>\n",
       "      <td>0.9590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.146396</td>\n",
       "      <td>0.95910</td>\n",
       "      <td>0.147174</td>\n",
       "      <td>0.9599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.142612</td>\n",
       "      <td>0.95960</td>\n",
       "      <td>0.143386</td>\n",
       "      <td>0.9616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.138699</td>\n",
       "      <td>0.96158</td>\n",
       "      <td>0.140399</td>\n",
       "      <td>0.9621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.135310</td>\n",
       "      <td>0.96176</td>\n",
       "      <td>0.139051</td>\n",
       "      <td>0.9630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.131848</td>\n",
       "      <td>0.96296</td>\n",
       "      <td>0.135376</td>\n",
       "      <td>0.9628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.128667</td>\n",
       "      <td>0.96424</td>\n",
       "      <td>0.133579</td>\n",
       "      <td>0.9638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.125667</td>\n",
       "      <td>0.96508</td>\n",
       "      <td>0.131282</td>\n",
       "      <td>0.9645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.122625</td>\n",
       "      <td>0.96568</td>\n",
       "      <td>0.129443</td>\n",
       "      <td>0.9649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.119621</td>\n",
       "      <td>0.96708</td>\n",
       "      <td>0.127192</td>\n",
       "      <td>0.9657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.116971</td>\n",
       "      <td>0.96768</td>\n",
       "      <td>0.126675</td>\n",
       "      <td>0.9660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.114422</td>\n",
       "      <td>0.96826</td>\n",
       "      <td>0.123106</td>\n",
       "      <td>0.9666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.111664</td>\n",
       "      <td>0.96952</td>\n",
       "      <td>0.122593</td>\n",
       "      <td>0.9663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.109165</td>\n",
       "      <td>0.96996</td>\n",
       "      <td>0.121194</td>\n",
       "      <td>0.9670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.106975</td>\n",
       "      <td>0.97024</td>\n",
       "      <td>0.118273</td>\n",
       "      <td>0.9680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.104566</td>\n",
       "      <td>0.97110</td>\n",
       "      <td>0.117657</td>\n",
       "      <td>0.9678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.102409</td>\n",
       "      <td>0.97198</td>\n",
       "      <td>0.114924</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.100103</td>\n",
       "      <td>0.97220</td>\n",
       "      <td>0.113438</td>\n",
       "      <td>0.9691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.97334</td>\n",
       "      <td>0.111955</td>\n",
       "      <td>0.9690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.095984</td>\n",
       "      <td>0.97366</td>\n",
       "      <td>0.112349</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.094037</td>\n",
       "      <td>0.97418</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>0.9693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.092239</td>\n",
       "      <td>0.97456</td>\n",
       "      <td>0.109178</td>\n",
       "      <td>0.9706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.090367</td>\n",
       "      <td>0.97540</td>\n",
       "      <td>0.107044</td>\n",
       "      <td>0.9704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.088610</td>\n",
       "      <td>0.97582</td>\n",
       "      <td>0.106586</td>\n",
       "      <td>0.9702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.97622</td>\n",
       "      <td>0.105657</td>\n",
       "      <td>0.9704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.085073</td>\n",
       "      <td>0.97686</td>\n",
       "      <td>0.104468</td>\n",
       "      <td>0.9709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.288308   0.68434  0.608498        0.8626\n",
       "1   0.519868   0.86940  0.393318        0.8995\n",
       "2   0.399237   0.89168  0.332290        0.9101\n",
       "3   0.351163   0.90344  0.302568        0.9172\n",
       "4   0.322431   0.90940  0.283337        0.9209\n",
       "5   0.301455   0.91544  0.266799        0.9247\n",
       "6   0.284856   0.91994  0.255297        0.9280\n",
       "7   0.270638   0.92468  0.244445        0.9317\n",
       "8   0.258302   0.92758  0.234849        0.9329\n",
       "9   0.247264   0.93062  0.226034        0.9360\n",
       "10  0.237008   0.93324  0.220393        0.9384\n",
       "11  0.227935   0.93614  0.211275        0.9409\n",
       "12  0.219721   0.93834  0.205408        0.9419\n",
       "13  0.211464   0.94020  0.198574        0.9463\n",
       "14  0.204418   0.94276  0.193185        0.9479\n",
       "15  0.197701   0.94410  0.185948        0.9500\n",
       "16  0.191090   0.94604  0.180749        0.9519\n",
       "17  0.184971   0.94792  0.176808        0.9527\n",
       "18  0.179248   0.94938  0.171794        0.9537\n",
       "19  0.173538   0.95100  0.167493        0.9542\n",
       "20  0.168472   0.95250  0.165285        0.9548\n",
       "21  0.163643   0.95356  0.160126        0.9567\n",
       "22  0.159010   0.95520  0.156726        0.9576\n",
       "23  0.154627   0.95644  0.153484        0.9597\n",
       "24  0.150432   0.95774  0.149473        0.9590\n",
       "25  0.146396   0.95910  0.147174        0.9599\n",
       "26  0.142612   0.95960  0.143386        0.9616\n",
       "27  0.138699   0.96158  0.140399        0.9621\n",
       "28  0.135310   0.96176  0.139051        0.9630\n",
       "29  0.131848   0.96296  0.135376        0.9628\n",
       "30  0.128667   0.96424  0.133579        0.9638\n",
       "31  0.125667   0.96508  0.131282        0.9645\n",
       "32  0.122625   0.96568  0.129443        0.9649\n",
       "33  0.119621   0.96708  0.127192        0.9657\n",
       "34  0.116971   0.96768  0.126675        0.9660\n",
       "35  0.114422   0.96826  0.123106        0.9666\n",
       "36  0.111664   0.96952  0.122593        0.9663\n",
       "37  0.109165   0.96996  0.121194        0.9670\n",
       "38  0.106975   0.97024  0.118273        0.9680\n",
       "39  0.104566   0.97110  0.117657        0.9678\n",
       "40  0.102409   0.97198  0.114924        0.9688\n",
       "41  0.100103   0.97220  0.113438        0.9691\n",
       "42  0.098062   0.97334  0.111955        0.9690\n",
       "43  0.095984   0.97366  0.112349        0.9688\n",
       "44  0.094037   0.97418  0.109550        0.9693\n",
       "45  0.092239   0.97456  0.109178        0.9706\n",
       "46  0.090367   0.97540  0.107044        0.9704\n",
       "47  0.088610   0.97582  0.106586        0.9702\n",
       "48  0.086643   0.97622  0.105657        0.9704\n",
       "49  0.085073   0.97686  0.104468        0.9709"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABNK0lEQVR4nO3deZwUxf3/8Vd1zz2z932x3JecgghGFDFGNCqaRI0xRjFqNInJ92suY0xiEvWXaO581QQTzxzGO0aJxgOCBFQQOZT7ZhfY+96du35/9Oywy+7CAAO7DJ/n4zGP7unp6akp2H1vVVdXK601QgghhOg/Rn8XQAghhDjZSRgLIYQQ/UzCWAghhOhnEsZCCCFEP5MwFkIIIfqZhLEQQgjRzw4ZxkqpR5RS1UqpD/t4XSmlfquU2qKUWqOUOjX5xRRCCCFSVyIt48eAOQd5/QJgROxxE/DQ0RdLCCGEOHkcMoy11ouB+oPsMhd4QlveATKVUkXJKqAQQgiR6pJxzrgE2N3leUVsmxBCCCESYDueH6aUugmrKxu32z2lrKwsaceORqMYhoE/DPvaoxR6DVxm0g5/UumsS3H0pC6TR+oyeaQuk+Nw63HTpk21Wuu83l5LRhhXAl1TtTS2rQet9XxgPsDUqVP1ihUrkvDxlkWLFjFr1ixW7mrgUw8u5bF5pzFrVH7Sjn8y6axLcfSkLpNH6jJ5pC6T43DrUSm1s6/XkvGn0UvAF2KjqqcDTVrrvUk47hFx263msD8U6a8iCCGEEIflkC1jpdTfgFlArlKqAvghYAfQWv8eWABcCGwB2oF5x6qwiegM4w4JYyGEECeIQ4ax1vqqQ7yuga8krURHye2IhXEw2s8lEUIIIRJzXAdwHQ8uaRkLIUTqioQhEoBIEKIRiIb3L3Wkj23R7q933S8ShEio+3okBNEQGDaYfstx+VopF8bxbupguJ9LIoQQA0AkDGG/FTThAEQCuNv3Qu0W0NHeH5EQhNqt94XaIdSx/xHusI6jo/uDTesu69EDAjEWdNGwVZZo10fkgIAMW8GpI7FQ3F9mwkFrqY9jr6czQ8L4SDlsBjZDSctYCHFsdAZPNNSlRdW5DFiBFWyHUFtsva1LoLV3b7XF17s819H9oaZ1PNx0JIIOR9DhICoSgojfKkM4gIoGreeRIISD6GCAaCCIDgbRYU00otBRhY4odARO0YqWFwENaIXWsXVAR9X+76oANEodUAcKMAyUMkAZYBqgFEopMGLbsKEx0BiAidYmGgXaRGvDKlNYEQ3rWF5rdAiiIU00pNFRG8pwgJEOhokyjNixTeuzTRM6P6+zLPF9DGt/FFrr2Hfrvo7W6KiGqEZHoujYkmjUeh6OYHi9DDmG/5W6SrkwBqt1LOeMhUghkRBmuB3aaiEcQIf9EOhA+9vQ/g50oB0d7EAHY2EUDkIoZC3DIXQoAKEg2t9GtKPd2j/QHntvAB3wo4N+CEfQ0QhEIujo/hC0ujmj+39pR4GoiuVlbBlV1i96BUrp2JLugaaI76ujCq0NdNSwAjG2LRoLTB1V8Z7VzkdiTMATe/S3aOxxkJ5Kmw3D7Y4/lMeD4XOjbDYrMHUUNLF/j2gsUDWEImhi4Rp7DSKxgI09j4cz+8NaKTAUCgU2G8q0oWwmymbDsNmskI8tDZ/v+FQTKRrGLocpLWMhjkA0GCTa2kq0tZVISwvR1jaiba3oYAgdCUM4jA4G0P5W8Leg/a1WIAb86FAIHQqjgyGioRA6GI4/15HOFl8UhfVLk2gUxf5uSh0KEQ2FY++LEA1H0WEda9lBflSxIR6CBzbVjhWDbleAGgplGtbDZoBpxtZN65e4aVrZE8uL+DKqY8ECym5D2R0ou73Hw7DZMJ1OlNOJEVsqpxPD5UQ5Ys9NE9BWOHUGkY6FVNRq3iqnI/Z+l7XucsXeb23/YM0aTp061WpBmtb3QKlYa9MKL6sFGfsCnQ/YH4Za7w/HqI635DvLpWxWfajOgDNN63vG1g2Xywpfu/04/VsObCkZxm67KdcZixOC1hrCYatLMRSMtdIC6GDQ2hbwE2luJtrcTKSpiUhTbNncRKSpiWhTMzoatX7B22yxoFCxByhDQTRshWXATzQY+4xgyArPYIhoKELUHyYaiKAj+ui+kKExDI0ytfU73dQow1q3vnDnQmGFnLXUKAybgbKbKIcTw2fDdNhRDgeG04FyOGj2+8nKzbWCzBF72GMB5XCCzYGyO6xWjc2BstnBZo9ts15TnnSUy2O9x27vchx7t6BQSlkBZcS6Ozu3pYhQayvuCRP6uxiii5QN446ghLFInI5G40EY7QzEQCB+no5waP96JGyth8LocMhqLYZjrcBwl22hEN71G9i3dCnR5harpdncTKS5yVq2tBBt74h1ryXOcCpMp8J0akx7FFTsXGIEop3dn/Gu0Nh7TKxQ7AxHE0ybgXKbqDQTw2liOu0YThPDZWI4bZguG4bLjuGyodw+K8jcGeDOQHkzUZ5M8GShfFkol9cKNdOG1ScY6w5UscA1DLB7we62Hubht4Y2LlrEKJk1SqSolAxjl8OkXVrGKUkHg0Q6W4nNzURbWog0txBtjS1bmuPLaIffGsDS+QiF0KGg1ToMhqxtnaEbCh2T8vqAJrvGcFjBadoj2B1RTKfG8EUx7BrD7N6SjLcsTQPlcmF6rYfhc2P6fCinBxxdgs3uBYcH7J3buy49+193ePev29xWQAohBoSUDGO33cAvLeMBSWtNpK6OcG0t0bY269HaSqStLXZ+snNby/4u2S4P3d5+8A+w2zHT0zG9HpTTgbIpDFNZLUMnKLdGGSYKjVIKZdgwDBdKhVEEMVQQRRAV9WPgB8MajKNivapKxUJT6f3PbaYVkE6P1UJ0eVBOH8rto7qljcKywVZo2ly9LD3gTAOnL7ZMt5YOn7VPCnWNCiH6lpJh7HHYqGkJ9HcxTlpaayKNjQR37CC4cyfBnTsJ7dxJcIe1Hm1rO+j7lduF6XFheN2YPjd2nwNXfh6muxDTZWC4FKZDY9rCGEYHpurAUK2YuhkVbEQFdxE/OdkXR9r+EHR4wZFhBaDDG3vE1p1pPR8OX/d1m7PP0NywaBGF0rUqhDiElAxjt11GUx+VYJBQZSXh+gYiDfWE6+uJdF1vbEJ3dBANBtCBINrv777u96P9/v3HMxT2/GwcBVlknDESR44Lm0dj0I4ZbcHQzRjhBoxQHQYd+wf79EmB8oI9A1yZ4MoA1yBwd653PjLBlX7Atgyr9WnI/TWFEANHSoaxSwZwdaODQcK1tYSrqwlVVxOpryfS0EC4oYFIQ+P+543W84KODrb0diCbiS3di+l1YtgVytAYRgTDjKA8IZQnhEEApf3Y3CEcvjCOtDAObwRlHnBXzaALvHngyQFvIXjHgTcXPLmx7dndz3F2O9/pku5bIURKSckwdjuMlLu0SWtNpLaWcE0N0Y4Oa3CS31pG/R1WS7XDT7S9nXCdFbzh6hrCsfDtjeH1YKb7MNNcmB47znIf5igfHf56MjJNbEYLJo3YaMR0WoONumWgzQ3uLCs43bnWevyR2aUl2rVlGmupSqAKIURcaobxCdhNrYNBQtU1hPfuIbTHegQrKwnv2UOocg+hvXut2YUOxTCwZWVgy0rDnuHCPb4AmzsfmzOA3daGTTVgUo9NtaB666m1e+kwfbjzBoOvDHwFBzzyrYcnxxpgJIQQ4qildBhrrQfEhfo6FCK0bx+hykpC+/YR3ldFqGof4apqwvv2EaqqIlJX1+N9Zm4u9uJinGPG4DtnFvZMBzZXCFN1oMItGOEmVKgBI1SPEahBBWpQUT/Wbae7sHshrRDSiiBtmBWq3lh3sDcPvPmx57ng8PLuokXMkkFHQghx3KRmGDtsaA2BcDR+S8VjKRoMEt6712rRVlbGl8HKSkKVewhXVfWY2MHMyMBWWIitsADXKadgKyzAXliILdOD3RXAbmvAaN4OtZuhdjHUb4d9B7T23dlWsOYUgG9ErNVaYIVu5zKtwBr1K4QQYsBKzTC2W8NxO4KRpIVxqKqKwIYN8S7kUOWeeOiGa2vj87YCVldxYQGO4hK806ZhLymJPYqxFxVhy0rDaN8DdVusR/02qHsHtm6B9i4tZNMBOcOh4BQYeynkjoScYVYr15sPNkdSvpsQQoj+lZph7Ijd0zgUIesI3q+jUQJbttCxciXtK1fS8f5KQpVdRgPb7diLirAXF+OdORN7cfH+R2kp9oJ8a/JzraFhO+xZBXtXwabHYelmaD5gZHFakRW6Yy62lrkjIXcEZJbLJThCCHESSMkw7mwNJzqIS2uN/8MPaVu6jPaV79PxwSqizc2Add7WM3kyWdd8Hvf48dhLS7Hl5cXuldlFNBoL3mWwbrUVvntXg7/Jet2wQ/4YGDzTCtycYdYye6g18YQQQoiTVkqGsbszjA9xrXFg82aaXn6F5gULCO3eDYBj6FDSz/8E7lOn4Dl1MvZBg3oOAotGoHqDFbadj31rIGAFOKbD6lo+5VNQPAmKJkL+WGumJiGEEOIAqRnGjr5bxsGKCppfWUDzK68Q2LQJDAPv9Onk3nwzvtnnYMvqpWM7GoGtb8Gm12LBuxbCHdZrNjcUjoMJV1ihWzQJ8kbL+VwhhBAJS80wPqBlrLWm8elnaHr+eTpWr7b2mTyZgjvvJH3O+dhyc3s/UP12+ODPsOqv0LLHmoe4cAJMnRcL3omQMwLMlKxGIYQQx0lKpsiBLeO6P8yn5te/xjlyJHnfuI30Cy7EUVrS+5tDHbDuJfjgSdjxtnU/1uEfhwt+CiMvkBavEEKIpEvNMI61jP2hCC1vvEHNr39N+sUXU3zfz/qeBKR6Pbz3MKx9FgJNkDUYZt8JEz8HGX0EtxBCCJEEqRnGsZax3rKZynu/g2vCBIp+8uPegzgahXcehDfusi4jGjsXJn8eys+Um68LIYQ4LlIzjO0mGYEWBv/qPkyfj9L/+x2Gy9Vzx9YaePEW2PI6jL4ILv4teHOOf4GFEEKc1FIyjJ1EufO9J7A1N1L6179gz8/vudO2RfD8TdDRCBf+HE67Qe4iJIQQol+kXBhrrWm4527G1W3ng+u+ybjx47rvEAnBwnthya+sma4+/7x1aZIQQgjRT1IujBuefJKm557j2THnocbNOODFnfDcF6FiOUy+Bi74mXXTeiGEEKIfpVQYt769hKqf/gzfx8/lpYJP8omuk36s+wf841ZAw2cegXGf7rdyCiGEEF2lzHBhc98+Km+7DeeIEZT87Gc4Hfb9M3A174VnrrPmg/7SYgliIYQQA0pKhHGkqYnMBx9C2e2UPfgAhteL227i7wzjqo9AR+ETd0P2kP4trBBCCHGAlOimbnrpn5h1dZQ+8Tj2EmuCDrfD3H+jiJoN1jJvdD+VUAghhOhbSoRx1uev5iPTZOyUKfFtLrtJe9cw9uTKNcRCCCEGpJToplZKESku6ratWzd1zUZpFQshhBiwUiKMe+NxmNYALq2tlnHeqP4ukhBCCNGrlA1jtz0Wxi17IdAM+WP6u0hCCCFEr1I2jF0Ok45gtMvgLWkZCyGEGJhSYgBXb+LnjGs2WxvknLEQQogBKmVbxp3d1Lp6A7izwJvX30USQgghepW6YewwiUQ1umaD1SqWOzIJIYQYoFI2jF12E5CR1EIIIQa+lA1jj8MkjyYMfyPkyUhqIYQQA1fKhrHbbjLcqLSeSMtYCCHEAJayYeyym4xQFdYTGUkthBBiAEvZMHY7TEaoSsKONEgr7O/iCCGEEH1KKIyVUnOUUhuVUluUUrf38vogpdRCpdQHSqk1SqkLk1/Uw+O2m4wwKmlPHy4jqYUQQgxohwxjpZQJPABcAIwFrlJKjT1gtzuBp7XWk4HPAg8mu6CHy203Ga4qaUkb1t9FEUIIIQ4qkZbxNGCL1nqb1joIPAXMPWAfDaTH1jOAPckr4pHxRhrIVc00eCWMhRBCDGyJTIdZAuzu8rwCOP2Afe4C/q2UuhXwAh/v7UBKqZuAmwAKCgpYtGjRYRa3b62trd2Op6rWMBRYXWenNomfczI4sC7FkZO6TB6py+SRukyOZNZjsuamvgp4TGv9C6XUDOBJpdQ4rXW0605a6/nAfICpU6fqWbNmJenjYdGiRXQ9XtuSjbAeHCNmMmvW9KR9zsngwLoUR07qMnmkLpNH6jI5klmPiXRTVwJlXZ6XxrZ19UXgaQCt9TLABeQmo4BHytmwmVbtos7o12IIIYQQh5RIGC8HRiilhiilHFgDtF46YJ9dwLkASqkxWGFck8yCHi6zbiNbdAkdoeihdxZCCCH60SHDWGsdBr4KvAasxxo1/ZFS6sdKqUtiu30DuFEptRr4G3Cd1lofq0InQtVuZBuldIQi/VkMIYQQ4pASOmestV4ALDhg2w+6rK8DPpbcoh2F9nporWKneR4dQQljIYQQA1uyBnANLLWbAKi0DQZpGQshhBjgUnM6zOr1AOxxDJJuaiGEEANeaoZxzUawe2hxFuKXbmohhBADXGp2U9dsgNyRuLRdWsZCCCEGvNRtGeeNxmU3aZeWsRBCiAEu9cLY3wQteyBvFG67iV9axkIIIQa41AvjGmskNfljcDtM6aYWQggx4KVgGG+wlnmj8DhMuc5YCCHEgJeaYWxzQWY5Lru0jIUQQgx8qRnGuSPAMOWcsRBCiBNCCoaxNZIawG03CUU0oYjcLEIIIcTAlVphHGiBpt2QNwoAt8MEkK5qIYQQA1pqhXFsTurOlrHLboWxzMIlhBBiIEutMK7ZaC3zxgDgkZaxEEKIE0CKhfEGMB2QNRiwzhmDhLEQQoiBLbXCuHoD5IwA05py29XZMpZuaiGEEANYaoVxzYb44C2QlrEQQogTQ8qEsRHxQ+Ou+OAt6BLG0jIWQggxgKVMGHvaKwHdvWUsA7iEEEKcAFImjL1tu62V/DHxbdIyFkIIcSJImTD2tO8GwwbZQ+PbOlvGMiWmEEKIgSxlwtjbthtyhoNpj2+TAVxCCCFOBCkTxp72Xd3OF8P+Gbg6gjI3tRBCiIErNcI41IG7o6rbSGoA01A4bIa0jIUQQgxoqRHGdVtQRHu0jMHqqu4IhvuhUEIIIURiUiOMD5iTuiu33ZSWsRBCiAEtNcI4azCVxRdAzrAeL7kdJh0hOWcshBBi4EqNMC6dyuaRN4PN2eMlq5taWsZCCCEGrtQI44NwO0y5zlgIIcSAlvphLOeMhRBCDHApH8Yuu0m7dFMLIYQYwFI+jKWbWgghxECX+mFsN2QAlxBCiAEt5cPY47DJOWMhhBADWsqHsUsGcAkhhBjgUj6M3XaTYDhKJKr7uyhCCCFEr1I/jB3WV5RBXEIIIQaq1A/j2G0U5fImIYQQA1XKh3HnPY2lZSyEEGKgSvkwdjusMJZBXEIIIQaqlA9jT2cYSze1EEKIASrlw7izm1paxkIIIQaqlA9jt4SxEEKIAc7W3wU41tzSTS2ESHGhUIiKigr8fn9C+2dkZLB+/fpjXKrU11c9ulwuSktLsdvtCR8r9cPYLmEshEhtFRUVpKWlMXjwYJRSh9y/paWFtLS041Cy1NZbPWqtqauro6KigiFDhiR8rIS6qZVSc5RSG5VSW5RSt/exzxVKqXVKqY+UUn9NuARJUNlaybut7xKOhnu8Jt3UQohU5/f7ycnJSSiIxbGllCInJyfhXopOhwxjpZQJPABcAIwFrlJKjT1gnxHAd4GPaa1PAf7nsEpxlN7b+x5/rvsze1v39njN5ZDrjIUQqU+CeOA4kn+LRFrG04AtWuttWusg8BQw94B9bgQe0Fo3AGitqw+7JEdhUPogAHa17OrxmnRTCyGEGOgSCeMSYHeX5xWxbV2NBEYqpf6rlHpHKTUnWQVMxKC0vsPYbhrYTSXd1EIIcQz5fL7+LsIJLVkDuGzACGAWUAosVkqN11o3dt1JKXUTcBNAQUEBixYtSsqHa61x4GDpuqUU7SvqWTil2bx9J4sW7UvK56W61tbWpP3bnOykLpNH6rJvGRkZtLS0JLx/JBI5rP0TdSyOOZAdrB79fv9h/X9NJIwrgbIuz0tj27qqAN7VWoeA7UqpTVjhvLzrTlrr+cB8gKlTp+pZs2YlXNBDyftrHtGMKL0dM+2/b5CTn8+sWROS9nmpbNGiRb3Wozh8UpfJI3XZt/Xr1x/W6OhjNZo6LS0NrTXf/va3+de//oVSijvvvJMrr7ySvXv3cuWVV9Lc3Ew4HOahhx7ijDPO4Itf/CIrVqxAKcX111/P//7v/ya9XMfKwerR5XIxefLkhI+VSBgvB0YopYZghfBngc8dsM+LwFXAo0qpXKxu620JlyIJ8ux57Gru2U0N1rXG0k0thDgZ/OifH7FuT/NB94lEIpimmfAxxxan88OLT0lo3+eff55Vq1axevVqamtrOe200zjrrLP461//yvnnn8/3vvc9IpEI7e3trFq1isrKSj788EMAGhsbEy5TqjnkOWOtdRj4KvAasB54Wmv9kVLqx0qpS2K7vQbUKaXWAQuBb2mt645VoXuTa8ulorWCSLRn6LrtpgzgEkKI42DJkiVcddVVmKZJQUEBZ599NsuXL+e0007j0Ucf5a677mLt2rWkpaUxdOhQtm3bxq233sqrr75Kenp6fxe/3yR0zlhrvQBYcMC2H3RZ18BtsUe/yLPlEY6G2de+jxJf9/Fl0jIWQpwsEmnB9sekH2eddRaLFy/mlVde4brrruO2227jC1/4AqtXr+a1117j97//PU8//TSPPPLIcS3XQJEyc1Pn2fMA2Nm8s8drbrsp1xkLIcRxMHPmTP7+978TiUSoqalh8eLFTJs2jZ07d1JQUMCNN97IDTfcwMqVK6mtrSUajfLpT3+au+++m5UrV/Z38ftNykyHmWezwnh3824o7v6a227S7A/1Q6mEEOLkctlll7Fs2TImTpyIUor77ruPwsJCHn/8ce6//37sdjs+n48nnniCyspK5s2bRzQaBeD//b//18+l7z8pE8bpZjou09XrtcYuh0m7nDMWQohjprW1FbBmn7r//vu5//77u71+7bXXcu211/Z438ncGu4qZbqpDWVQmlba5yxcfgljIYQQA1TKhDFAeXp5r5c3ue0ygEsIIcTAlVJhPChtELtbdve4vElGUwshhBjIUiqMy9LLCEVDVLd3v0+FNZo6SjSq+6lkQgghRN9SKoz7umGEO3YbxUA4etzLJIQQQhxKSoVxeXo50PNa4/htFKWrWgghxACUUmGc78nHYTjY3bK72/bOMG4PhvujWEIIIcRBpVQYG8qgLK2sx4hqV6ybWmbhEkKIE1s4nJqNqpQKY7AGcfU4Z9zZTR2Uc8ZCCHGsXHrppUyZMoVTTjmF+fPnA/Dqq69y6qmnMnHiRM4991zAmiBk3rx5jB8/ngkTJvDcc88B4PP54sd69tlnue666wC47rrruPnmmzn99NP59re/zXvvvceMGTOYPHkyZ5xxBhs3bgSsu1F985vfZNy4cUyYMIHf/e53vPXWW1x66aXx477++utcdtllx6E2Dk/KzMDVqTytnGV7lhHVUQxl/a3hccg5YyHESeJft8O+tQfdxR0Jg3kYv/4Lx8MFPz3kbo888gjZ2dl0dHRw2mmnMXfuXG688UYWL17MkCFDqK+vB+AnP/kJGRkZrF1rlbOhoeGQx66oqGDp0qWYpklzczNvv/02NpuNN954gzvuuIPnnnuO+fPns2PHDlatWoXNZqO+vp6srCy+/OUvU1NTQ15eHo8++ijXX3994t/9OEm5MB6UPohAJEB1ezWF3kIAXDKASwghjrnf/va3vPDCCwDs3r2b+fPnc9ZZZzFkyBAAsrOzAXjjjTd46qmn4u/Lyso65LEvv/zy+D2Ym5qauPbaa9m8eTNKKUKhUPy4N998MzabrdvnXXPNNfz5z39m3rx5LFu2jCeeeCJJ3zh5Ui6My9LKANjdsjsexvu7qSWMhRApLoEWbMcxuIXiokWLeOONN1i2bBkej4dZs2YxadIkNmzYkPAxlFLxdb/f3+01r9cbX//+97/POeecwwsvvMCOHTuYNWvWQY87b948Lr74YlwuF5dffnk8rAeSlDtnPCg9dq1xl0Fc7ng3dWqe+BdCiP7W1NREVlYWHo+HDRs28M477+D3+1m8eDHbt28HiHdTn3feeTzwwAPx93Z2UxcUFLB+/Xqi0Wi8hd3XZ5WUWPetf+yxx+LbzzvvPP7whz/EB3l1fl5xcTHFxcXcfffdzJs3L3lfOolSLowLPYXYDTs7W/Zfa+xzWn8F1bUG+6tYQgiR0ubMmUM4HGbMmDHcfvvtTJ8+nby8PObPn8+nPvUpJk6cyJVXXgnAnXfeSUNDA+PGjWPixIksXLgQgJ/+9KdcdNFFnHHGGRQVFfX5Wd/+9rf57ne/y+TJk7uNrr7hhhsYNGgQEyZMYOLEifz1r3+Nv3b11VdTVlbGmDFjjlENHJ2B11Y/SqZhUppWat3XOCYvzUlJppsVOxq4YWY/Fk4IIVKU0+nkX//6V6+vXXDBBd2e+3w+Hn/88R77feYzn+Ezn/lMj+1dW78AM2bMYNOmTfHnd999NwA2m41f/vKX/PKXv+xxjCVLlnDjjTce8nv0l5QLY7CmxTzw8qYZw3J4c30V0ajGMFQf7xRCCJFqpkyZgtfr5Re/+EV/F6VPKddNDdYgrt0tu9F6/40hpg/NoaE9xMaqln4smRBCiOPt/fffZ/HixTidzv4uSp9SMozL08vpCHdQ01ET3zZjWA4Ay7bW9VexhBBCiF6lZBjH797UZUR1SaabQdke3tkmYSyEEGJgSckwLkvff61xV9OHZvPu9nq5r7EQQogBJSXDuMhbhE3ZetxKccawHJo6Qqzb29xPJRNCCCF6Sskwthk2StNKe4yonj7UOm8sXdVCCCEGkpQMY9g/orqrogw3g3PkvLEQQvS3rndoOtCOHTsYN27ccSxN/0vZMB6UPohdzbu6Xd4EVlf1u9vrich5YyGEEANESk76AVbLuD3cTp2/jlx3bnz79KE5/O293azb08z40ox+LKEQQiTfz977GRvqD35zhkgkEr8DUiJGZ4/mO9O+c9B9br/9dsrKyvjKV74CwF133YXNZmPhwoU0NDQQCoW4++67mTt3bsKfC9YNI2655RZWrFgRn2HrnHPO4aOPPmLevHkEg0Gi0SjPPfccxcXFXHHFFVRUVBCJRPj+978fn4JzoEvZMC5PLwesy5u6hvGM2HnjZdtqJYyFECJJrrzySv7nf/4nHsZPP/00r732Gl/72tdIT0+ntraW6dOnc8kll3S7O9OhPPDAAyilWLt2LRs2bOATn/gEmzZt4ve//z1f//rXufrqqwkGg0QiERYsWEBxcTGvvPIKYN1Q4kSRsmEcv9a4ZRenFpwa356f7mJonpdlW+u46axh/VU8IYQ4Jg7VggVoOQa3UJw8eTLV1dXs2bOHmpoasrKyKCws5H//939ZvHgxhmFQWVlJVVUVhYWFCR93yZIl3HrrrQCMHj2a8vJyNm3axIwZM7jnnnuoqKjgU5/6FCNGjGD8+PF84xvf4Dvf+Q4XXXQRM2eeODcjSNlzxkW+Ikxldpv4o9P0oTks39FAOBLth5IJIURquvzyy3n22Wf5+9//zpVXXslf/vIXampqeP/991m1ahUFBQU97lN8pD73uc/x0ksv4Xa7ufDCC3nrrbcYOXIkK1euZPz48dx55538+Mc/TspnHQ8pG8Z2w06xr7jH5U1gdVW3BsJ8uEeuNxZCiGS58soreeqpp3j22We5/PLLaWpqIj8/H7vdzsKFC9m5c+ehD3KAmTNn8pe//AWATZs2sWvXLkaNGsW2bdsYOnQoX/va15g7dy5r1qxhz549eDwePv/5z/Otb32LlStXJvsrHjMp200N+0dUH6jzeuNlW+uYVJZ5nEslhBCp6ZRTTqGlpYWSkhKKioq4+uqrufjiixk/fjxTp05l9OjRh33ML3/5y9xyyy2MHz8em83GY489htPp5Omnn+bJJ5/EbrdTWFjIHXfcwfLly/nWt76FYRjY7XYeeuihY/Atj43UDuO0QayuXo3WutuAgbw0J8PzfbyzrY5bZsl5YyGESJa1a9fG13Nzc1m2bFmv+7W2tvZ5jMGDB/Phhx8C4HK5ePTRR3vsc/vtt3P77bd323b++edz/vnnH0mx+13KdlODFcatoVYaAg09XpsxNIflO+oJyXljIYQQ/Sy1wzi9592bOs0YlkN7MMKaihNn6LsQQqSStWvXMmnSpG6P008/vb+L1S9SvpsarMubJuVP6vba6UOyAWue6inlWce7aEIIcdIbP348q1at6u9iDAgp3TIu8ZVgKKPXlnGOz8mogjSZp1oIIUS/S+kwtpt2irxFvV7eBFZX9YodDQTDct5YCCFE/0npMAarq7q3ljHA9KHZdIQirKloPL6FEkIIIbpI/TDu4+5NAKcPyUEp63pjIYQQor+kfhinDaIl1EJToOeo6Syvg9GF6SyT88ZCCHFcHex+xiej1A/j9P0jqnszfWg27+9sIBCOHM9iCSGEGADC4XB/FwFI8UubYP/lTTubdzIhb0KP12cMzeHR/+5g1a5GTo9NkymEECeqfffeS2D9we9nHI5EqD+M+xk7x4ym8I47DrpPMu9n3Nrayty5c3t93xNPPMHPf/5zlFJMmDCBJ598kqqqKm6++Wa2bdsGwEMPPURxcTEXXXRRfCavn//857S2tnLXXXcxa9YsJk2axJIlS7jqqqsYOXIkd999N8FgkJycHP7yl79QUFBAa2srt956KytWrEApxQ9/+EOamppYs2YNv/71rwF4+OGHWbduHb/61a8Srs/epHwYl6aVolDsbtnd6+vx88bb6iSMhRDiCCXzfsYul4sXXnihx/vWrVvH3XffzdKlS8nNzaW+vh6Ar33ta5x99tm88MILRCIRWltbaWjoOfNiV8FgkBUrVgDQ0NDAO++8g1KKP/7xj9x333384he/4Cc/+QkZGRnxKT4bGhqw2+3cc8893H///QA8+uij/OEPfziquoMEw1gpNQf4DWACf9Ra/7SP/T4NPAucprVecdSlSwKH6Tjo5U0ZHjtji9LlemMhREo4VAsWBv79jLXW3HHHHT3e99Zbb3H55ZeTm5sLQHa2NXnTW2+9xRNPPAGAaZpkZGQcMoyvvPLK+HpFRQVXXnkle/fuJRgMMmTIEADeeOMNnnrqqfh+WVnWBFGzZ8/m5ZdfZtCgQYRCIcaPH3+YtdXTIcNYKWUCDwDnARXAcqXUS1rrdQfslwZ8HXj3qEuVZGXpZexu7r1lDFZX9RPv7MQfiuCyJ951I4QQYr/O+xnv27evx/2M7XY7gwcPTuh+xkf6vq5sNhvR6P45JA58v9frja/feuut3HbbbVxyySUsWrSIu+6666DHvuGGG7j33nsZOnQo8+bNO6xy9SWRAVzTgC1a621a6yDwFNBbp/9PgJ8ByblzdBINShvEzpa+76M5Y1gOwXCUlbsO/peUEEKIviXrfsZ9vW/27Nk888wz1NVZPZmd3dTnnntu/HaJkUiEpqYmCgoKqK6upq6ujkAgwMsvv3zQzyspKQHg8ccfj28/77zzeOCBB+LPO1vbp59+Ort37+aZZ57hqquuSrR6DiqRMC4BujYrK2Lb4pRSpwJlWutXklKqJCtPL6cp0NTr5U0Apw3JxlDwjlxvLIQQR6y3+xmvWLGC8ePH88QTTyR8P+O+3nfKKafwve99j7PPPpuJEydy2223AfCb3/yGhQsXMn78eKZMmcK6deuw2+384Ac/YNq0aZx33nkH/ey77rqLyy+/nClTpsS7wAHuvPNOGhoaGDduHBMnTmThwoXx16644gpOP/30eNf10VK9TYbRbQelPgPM0VrfEHt+DXC61vqrsecG8BZwndZ6h1JqEfDN3s4ZK6VuAm4CKCgomNK1L/5otba29nnd2pr2NTxc8zDfLPwm5c7yXve5990Oqts1957pxmM/+OCCVHewuhSHR+oyeaQu+5aRkcHw4cMT3j8SiWAexmhq0dPll1/OLbfcwuzZs3t9fcuWLTQ1dW8AnnPOOe9rraf2tn8iA7gqgbIuz0tj2zqlAeOARbERcoXAS0qpSw4MZK31fGA+wNSpU/WsWbMS+PjELFq0iL6OV9pQysMvPUzmsExmDe99n6xhjVz24H9Z2pbL3Zce/cn4E9nB6lIcHqnL5JG67Nv69esPa0DWsRjAdbJobGxk2rRpTJw4kdmzZ/dZjy6Xi8mTJyd83ETCeDkwQik1BCuEPwt8rvNFrXUTEG/XH6xl3F8GZwymxFfC3zf+nUuG9T6sfmJZJtedMYRH/rudSyeVMHVwdj+UVAghTh5r167lmmuu6bbN6XTy7rsDbhxwXGZmJps2bQKsP2qS5ZDnjLXWYeCrwGvAeuBprfVHSqkfK6UuSVpJjiGbYePG8TeytnYtSyqX9LnfNz4xkpJMN999fq3MyCWEOKEc6pTjQNR5P+Ouj4EcxIk6kn+LhKbD1Fov0FqP1FoP01rfE9v2A631S73sO2sgtYo7XTLsEoq9xTy0+qE+K8rrtHH3pePYXN3KH/6z7TiXUAghjozL5aKuru6EDORUo7Wmrq4Ol8t1WO9L+Rm4OtlNOzdOuJEfLfsRSyqXMLN0Zq/7nTM6n4smFPF/b23hkxOKGJYnA0aEEANbaWkpFRUV1NTUJLS/3+8/7LAQPfVVjy6Xi9LS0sM61kkTxgBzh83l4TUP89Dqhziz5Mw+p2T7wcVjWbyphu8+v5anbpyOYZzco6uFEAOb3W6PzxqViEWLFh3W4CLRu2TWY8rftamrztbx2tq1/HfPf/vcLz/Nxfc+OYb3ttfz9Iq+Z+4SQgghkuGkCmOwWsfF3mIeWtX3uWOAK6aWcfqQbO5dsJ7qlgE3qZgQQogUctKFsd20c8OEG1hTu+agrWOlFPd+ajz+UJQf/3Ndn/sJIYQQR+ukC2OAS4ddSpG36JCt42F5Pr46ezgvr9nLWxuqjmMJhRBCnExOyjDuPHd8qNYxwM1nD2NEvo87X/iQtkD4OJVQCCHEyeSkDGPo0jo+yHXHAA6bwU8/PZ49TX7uf23jcSyhEEKIk8VJG8bx1nHNGpbuWXrQfaeUZ/OFGeU8tnQHDy7acpxKKIQQ4mRx0oYx7G8dP7j6wUPOXPP9i8ZyycRi7nt1I/e9ukFmuhFCCJE0J3UY2007N4y/IaHWsd00+NWVk7hq2iAeXLSVH770EdGoBLIQQoijd1KHMcBlwy+j0Ft4yHPHAKahuPeycdx01lCeWLaTbz67mnAkepxKKoQQIlWd9GFsN+3cOP5GVtesZtmeZYfcXynFdy8YzTfOG8nzKyv5yl9Xyh2ehBBCHJWTPozBah0XeYv44bIfsqt51yH3V0px67kj+OHFY3ntoypueHwF7UG57EkIIcSRkTDGah3/bvbvCIQDzHt1Htubtif0vnkfG8J9n5nAf7fU8oU/vUezP3SMSyqEECIVSRjHjMoexZ/O/xNhHWbeq/PY0pDYJUxXTC3jd1edyuqKRq6a/w6VjR3HuKRCCCFSjYRxFyOyRvDonEcxlMH1r13PxvrEJvn45IQi5n9hKjtq25jzq8U8v7JCLn0SQgiRMAnjAwzNGMqjcx7FYTr44r+/yLq6xG4Scc6ofP719bMYU5TObU+v5uY/v09da+AYl1YIIUQqkDDuRXl6OY/NeQyvzcsNr1nXISdiUI6Hv900nTsuHM3CDTWc/+vFvL5ObjAhhBDi4CSM+1CaVspjcx4jw5nBTa/fxAfVHyT0PtNQ3HTWMP5565nkp7m48YkVfPvZ1bTI4C4hhBB9kDA+iCJfEY/NeYw8dx5fev1LLN+3POH3jipM48WvfIyvnjOcZ9+vYM6v3+adbXXHsLRCCCFOVBLGh1DgLeCR8x+h2FvMTa/fxG9X/pZAJLFzwQ6bwTfPH8UzN5+B3VRc9fA7fPf5NVQ3+49xqYUQQpxIJIwTkOfJ47E5j3HhkAt5eO3DfPqlT/Pe3vcSfv+U8iwWfH0m884YwjMrKjj7/kX88t8baZX7IwshhEDCOGGZrkzuOfMe5p83n6iO8sV/f5E7l9xJo78xofd7HDZ+cPFY3rjtbGaPyee3b23h7PsW8sSyHYRkfmshhDipSRgfphnFM3j+kue5YfwNvLLtFS558RL+ufWfCV9XPDjXywOfO5UXv/Ixhuf7+ME/PuITv1rMgrV75dpkIYQ4SUkYHwGXzcXXT/06T130FGVpZdyx5A6+9PqX2N28O+FjTCrL5KmbpvPIdVOxm4ov/2Uln3poKUu31kooCyHESUbC+CiMyh7FExc8wR2n38Ga2jVc9tJl/Gblb2gJtiT0fqUUs0cX8K+vn8V9n57AnsYOPvfwu1z24FJe/XCf3C9ZCCFOEhLGR8k0TK4afRUvzn2R2YNm88e1f+TC5y/kyXVPEowEEzyG4orTyvjPt87hJ5eOo64twM1/fp/zfvUfnl6+m2BYzikLIUQqkzBOkkJvIfeddR9PXfQUo7JHcd/y+7jkxUt4ZdsrRHViYeqym1wzvZyF35jFb6+ajMNm8u3n1nDWfQt5ePE2GX0thBApSsI4yU7JOYWHz3uYP3z8D/jsPm5/+3Y++/JnWbZnWcLHsJkGl0wsZsHXzuTx66cxONfDPQvWc8b/e5P7Xt0gd4YSQogUY+vvAqQipRRnlJzB9OLpvLLtFf7vg//jptdv4oziM7hh/A1MKZiCoQ79d5BSirNH5nH2yDxW7W7k94u28tB/tvL7/2xl9uh8Pnf6IM4emY9pqOPwrYQQQhwrEsbHkKEMLh52MZ8Y/Ame2vAUD699mOtfu55SXymXDL+EucPmUuwrTuhYk8oy+f01U9hd385Ty3fx9+UVvLF+BSWZbq6aVsYVU8vIT3cd428khBDiWJBu6uPAaTq59pRref0zr3PvmfdSklbCg6seZM5zc7jh3zfwz63/pCOcWNdzWbaHb50/mqW3z+bBq09lcK6Hn/97E2f89C1u+fP7LNlcK6OwhRDiBCMt4+PIbXNz8bCLuXjYxexp3cM/tv6Df2z5B3csuYN73r2HOYPnMHf4XCblTUKpg3c9O2wGF44v4sLxRWyvbeNv7+3imRW7+deH+yjJdDN3UjGXTS5hREHacfp2QgghjpSEcT8p9hVzy8Rb+NKEL/F+1fv8Y8s/WLB9Ac9tfo4SXwkXDrmQTw79JMMyhx3yWENyvdxx4RhuO28k/15XxfMrK/jD4m08uGgr40rSuWxyKRdPLCI/TbqxhRBiIJIw7meGMjit8DROKzyNO06/gzd3vckr217hTx/+iYfXPsyY7DF8cugnmTN4DgXegoMey2U3uWRiMZdMLKamJcA/V+/hhQ8q+cnL67h3wXrOHJ7Lp04t4byxBXgc8k8vhBADhfxGHkA8dk+8G7u2o5bXdrzGy1tf5ucrfs4vVvyCaYXTuHDohZxTdg5ZrqyDHisvzcn1Zw7h+jOHsKW6hRc+qOTFD/bw9adW4babzB6Tz0Xji5g1Kh+3wzxO31AIIURvJIwHqFx3LlePuZqrx1zNjqYdLNi+gFe2vcIPl/6QH6kfcWr+qZw76FxmD5p9yBHZw/PT+Nb5o/nGeaNYvqOel9fs5V8f7uWVNXvxOEzOHVPAJ8cXMWtUHi67BLMQQhxvEsYngMEZg/nypC9zy8RbWF+/njd3vclbu97iZ8t/xs+W/4wx2WOYPWg2swfNZkTmiD4HfxmG4vShOZw+NIcfXjyW97bX8/Lavbz64T7+uXoPXofJeWMLKFNhpgXD0pUthBDHify2PYEopRibM5axOWO5dfKt7GzeyVu73uLNXW/y4KoHeWDVA5SllXF26dnMLJ3J1IKpOExHr8eymQZnDM/ljOG5/PiSU1i2rY5X1uzl1Y/20dge4g9rX2fG0Bw+Piafc0bnU5rlOc7fVgghTh4Sxiew8vRy5o2bx7xx86hpr2Hh7oW8tfstnt74NH9e/2fcNjenF53OzJKZzCyZSZGvqNfj2EyDmSPymDkij59cOo6HX1xInbOIN9dX8f1/fAT/+IjRhWnMHp3PuWPymVSWJbN+CSFEEkkYp4g8Tx5XjLqCK0ZdQUe4g+X7lrO4YjFvV7zNot2LABieOTzeYh6VNYp8T36PLm27aTA2x2TWrLF8/6KxbK1p5a311by5oSp+uVSWx87Hhucyc0QuZ47IoyTTffy/sBBCpBAJ4xTktrk5q/Qszio9C60125u283bl27xd8TZPfvQkj374KACZzkxGZY9iVNao+HJoxtBuxxqW52NYno8bzxpKU3uI/2yuYdGGat7eUsvLa/YCMDTXy5kjcjlzeC4zhuWQ5rIf9+8shBAnMgnjFKeUYmjmUIZmDuXaU66lLdTGhvoNbKzfyMaGjWys38jfN/6dQCQAgM2wUWIrYf2q9ZxddjZjssfEW88ZHnv8OmatNZuqWnl7cw1LttTyzIoKnli2E9NQTCrLZOYIq+U8sTQTmymzrgohxMFIGJ9kvHYvUwqmMKVgSnxbOBpmZ/NONtZvZEPDBhZuWshDqx/iwdUPku/O56yys5hVOotpRdNw26wuaaUUowrTGFWYxg0zhxIIR1i5s5ElW2pYsrmW37y5mV+/sZk0p40Zw3KYOTKPmcNzKc/xHHKqTyGEONlIGAtsho1hmcMYljmMC7mQU1tOZfzp43m78m0WVyxmwbYFPLvpWZymk+lF05lZMpPxeeMZnjk8PlrbaTOZMSyHGcNy+Nb50NAWZOnWOpZsqWHxplr+va4KgNIsNzNH5HHGsBymDs6iKEPONwshREJhrJSaA/wGMIE/aq1/esDrtwE3AGGgBrhea70zyWUVx1GOO4dLh1/KpcMvJRgJsqJqBf/Z/R/+U2E9wArx4ZnDGZ09mtHZoxmbM5ZRWaPw2D1keR18ckIRn5xQhNaaHXXtLNlcw9uba3l59R7+9t4uAEoy3UwdnMXU8iymlGczqjBNRmoLIU46hwxjpZQJPACcB1QAy5VSL2mt13XZ7QNgqta6XSl1C3AfcOWxKLA4/hymgzOKz+CM4jO4fdrt7GrZxfq69ayvX8+G+g0srljMi1teBEChKE8vZ3zueCbmTWRS/iSGZw5nSK6XIblerpkxmHAkyrq9zazY0cD7OxtYtrWOf6zaA0Ca08akQZlMLc/m1PJMJpZlki4DwoQQKS6RlvE0YIvWehuAUuopYC4QD2Ot9cIu+78DfD6ZhRQDh1JW2JanlzNnyBwAtNZUtVexoX4D6+vXs75uPUv3LOWf2/4JgMfmYXxeLJzzJjEhbwITSjOZUJrJ9WcOQWtNRUMHK3bWxwP6129uQmtQCkbk+5hclsWp5ZlMHpTF8DwfhrSehRApRGl98BvRK6U+A8zRWt8Qe34NcLrW+qt97P9/wD6t9d29vHYTcBNAQUHBlKeeeuooi79fa2srPp8vacc7mSWjLrXW1IXr2B7YzvbgdrYHtrMnuIcoUQDybHkUO4optscejmJybbkYyhp53R7SbGuKsrUxwtbGKFubIrSFrGO7bTA0w2BYphlfpjkGZjjL/8vkkbpMHqnL5DjcejznnHPe11pP7e21pA7gUkp9HpgKnN3b61rr+cB8gKlTp+pZs2Yl7bMXLVpEMo93MjtWddkeauejuo9YVb2K9fXr2dywmTVNa9BYfxC6TBfDMocxImsEowtGc86pk7g5axQ2w2ZdL13bxspdjXywq4GVuxp5eVsz0djfkuU5HiaXWS3nSWWZjClKx2Hr/0uq5P9l8khdJo/UZXIksx4TCeNKoKzL89LYtm6UUh8HvgecrbUOJKV0IqV47J74vZs7dYQ72Na4jU0Nm9jcuJlNDZu6nYP22DxMyp/E5PzJTCmYwkUTx/OZKaUAtAfDrK1o4oPdVkAv3VrHi7Fzzw6bwdiidCaUZjCuJIPxJRmMyPfJNc9CiAEpkTBeDoxQSg3BCuHPAp/ruoNSajLwB6zu7Oqkl1KkLLfNzSm5p3BK7indtu9r28fKqpWsrLYeD656EI3GZtgYmzOWKflTGJU9iuGZw7m+fAgOcxhaa/Y2+fkg1npeU9nEc+9bk5EAuOwGY4rSmVASC+jSDIbl+bBLQAsh+tkhw1hrHVZKfRV4DevSpke01h8ppX4MrNBavwTcD/iAZ2ITOuzSWl9yDMstUlyht5ALh17IhUMvBKAp0MSq6lW8X/0+H1R9wJPrnyQcDQNgKpNB6YMYnjmcEZkjGJY5jM/NHM530kZgYmN7XRtrK5pYW9nE2oomnn2/gsdjAe0wDUYW+hhblG49ijMYU5QmU3oKIY6rhM4Za60XAAsO2PaDLusfT3K5hOgmw5nB2WVnc3aZNRwhGAmyo3kHWxu3srlhM1sat7CxfiNv7Hwjfg7aUAYFngJKfCXWo6yEa8aUUuQtIRrMoqLWxsZ9razb28wb66t5ekVF/PPKczxdAtp6FKa7ZPYwIcQxITNwiROSw3QwMmskI7NGcsGQC+LbO8IdbG/azpbGLexq3kVlayWVrZUs27OM6o7uZ1BcposhGUMYNnIYZ582jBxnGaGOfKrqPWzY28ZHe5r414f74vtneeyMLU5nTOH+gB6a6xsQA8WEECc2CWORUtw2N2NzxjI2Z2yP1/xhP3va9lDZYgX0zuadbGvaxvJ9y3l528vx/RyGgyEZQ5g2bShzvWWYkVza27KorvewZV+IJ9/ZSSBsXaJlMxSDc72MLPAxPD+NkQU+RhakMTjHKyEthEiYhLE4abhsLoZmDO1xm0iA1mAr25q2sbVxq/Vo2sqa2jW8tvM1ojoa38+X7WNieRlZjmLs0TzaOzw0tNhYVWvw7y0mkbAHHfFgw83gXB8Zys+KwEaG51u3ohyW78XjkB87IUR38ltBCMDn8Fkzg+VN6LY9GAlS2VrJruZd7GrZxa7mXexu2c2ulk3saV1EREesHTPBnbn/fQqDOrzUBNJYt7GAyJoCIoECooECCj2FDM9Pjwf00Dwvw/J85Kc55Zy0ECcpCWMhDsJhWl3WQzKG9HgtHA3TFGiiKdhkLQNNNAYau61/uOtD6rP2UNX+Qfx9HThZFylk5Y58QutziYYz0aEMXCqbwVlFDMvNYGiel6F5PobG5vT2OuVHVYhUJj/hQhwhm2Ejx51Djjunz30WBawZepqDzWxr3MaWxi3xx9bGrdR2LO+2/y6t2N2RxhtbMoiuT0eHM4mG0kmz5VLsK6A8s4hReaWMyM2iPMfL4FyPdHsLkQLkp1iI4yDdkc6k/ElMyp/UbXtrsJV9bfuoaq9iX9s+9rXvo6qtij2te6lo2UtNx1aCUT8hYCewswMW74LoNg86nI4OZ+Akh2xnIcXeUoZlljEmfwij8wsoz/aQ6bFL17cQJwAJYyH6kc/hY7hjOMOzhve5T2uwlar2Kqraq6hur6aieR9bGyqpbN5HTUc1TaE11LCUmiCsrgaqQUdcRIM5mNFs0mx5ZLszyfdmU5KWS3lWHsNz8hmZW0iuJxuH6Th+X1gI0SsJYyEGOJ/Dh8/hY1jmsD73aQm2UNlaybaG3ayr2cbm+p1Utu6hLrCXtsgmWqIBdrbA8hZgT/f3GtqFx8gly1lAoaeYwRkljMwtZ0xuOYPSS8l0ZkrrWohjTMJYiBSQ5khjdPZoRmeP5sJeMjsYCVLX0cC2+mo211axo6GGPc21VLXVU+evoyVcQ1PHHna1fcTyej9s3/9eQztxGVmk27PJdedS5CugPLOQwZmFFHjzyXPnkeHMwGPz4LK54rfBFEIkTsJYiJOAw3RQ5CugyFfAxwaN73UffyjCnsYONtfWsL5mJ1sbdlPZUkmNfx8toToqaGRP60esbXgHVRns87PshhOPzYPH7sZr9+CxeUhzpFHoLaTIW0Sxrzi+zPfkYzPk15AQ8lMghADAZTety6nyfJw/puelXP5QhMrGDnbXt7Olro6ttXvY2byPvS3VNPibaAu1o4wgQSNEmxFAqSB2exinI4RpqyRsrCWom7sd01Qm+Z58irxFpDvTSbOnWd3ydh9pDms9zZ5GmiONHYEd7G7ZTaYzE5/dJ13nIqVIGAshEuKym9YsYnk+ZpEPjOn2uj8UoarZz55GP/uaO9jb5Gdvo99aNlnPW9rbUPZGDHsDhr0RZW+gyt1MvbMZ01YDyk+YdkK6A020Rxl+8fwvALApGxnODDKdmd2WBwZ41/U0RxoZzgwJcjEgSRgLIZLCZTcpz/FSnuPtc5+OYCQezHsaO+JBvafRT1WDn6pmPw3tIUCDEUQZfpThx+kM4La3k5MJHncAu6Md09aB1m20+Fupbd9Be7iFtnArHeGOg5bTpmykO9PJdGZ2C/NMZyYO04FpmNiUDZthw1QmNmP/eroznSJvEYXeQnJcORLqImkkjIUQx43bsb8rvC/+UITq5gBVLX72NVkBva/Jz4dbdwEZVFcHqGkO0BII93ivwzTITbORlx4lyxcl3RvG5wnjdgaxOwIosx2t2vBHW+KzpO1u2c1HtR/RGGgkGO37XHiPzzIc8fPghd5CinxF5LhysBt2TMPEVGa39c5Q99g8uG1ua2l3y8A3AUgYCyEGGJfdZFCOh0E5nm7bFy2qZtasGfHn7cEw1c0BqlsCVLf4qWq2ltWxZUVtgKptflr8BuAAfIA1W5rdVOT5nOSluyhIc3JKupP8HBe5PifZXpNMr40sj0mmx8RhgwgRwtEwjYFG9rbuZW/bXva17WNvm7W+bO8yatpr4vfSPhJumxu3zU2WM4tcTy657lzy3HnkunPjjzx3HunO9HjL3W7asSkbpmEe8eeKgUHCWAhxQvI4bAzOtTE4t+9ucbC6xnuG9f71HXVtvLejnsb2UK/vd9kNcrxOctOc5Hgd5HizyfYVkut1Uu5zkF3oINfrJN1jYNrasZmacDRMREeIRCOEoqFu6/6wn/Zwu/UItdMR7oivt4XaaAw0UtNRw6rqVdS01yTUWleoeMvbZbrIcmVZU7W6cnpdVoeqqWmvwWO3WunSKu9/EsZCiJTmdhz6XDZAIByhrjVIXWuQ2rZAbD1Abau1XtMaoKrZz/q9zdS1BglGeg4wA0hz2shNc5Lrc5Drc5KX5iTX5yTX5yPX5yDH56DI6yTb6yDdZTvoeWetNS2hFmrba6ntqKWmo4bWYCthHSYUCVnLaIhwNEw4aq0HwgHq/db14+vq1lHnr6Mt1Nbj2D955if768jmxmv3xi5J88SXndu8du/+fewenKYz/gdGREeI6mi3dY0m05lJrjuXHHeOtXTl4LK5EvxXO/lIGAshBOC0mRRnuinOdB9yX601rYEw9W1BaluD1LcFqW8LUNsapKbFCvDa1gCbq1tZurWOpo7eW912U5HlcZDttUI62+sk22Mnw+Mgy2Mn02Mn0+Mg051FnqeAkRl20l12DOPwBo75w37q/HXUddRR769n+erllI8opz1ktdDbQm3x1nnntgZ/A5WtlbSF2ugIddAWbut2b+8j4bP74i30zvPkNmXDUAamYWIow1pXJi6biwxHBhnO2MORQbozPb6e5kiLD6wzlHHCD6aTMBZCiMOklCLNZSfNZT9kixsgGI5S1xagtiVIXVsgFt5B6tqC1LfGlm0B1jY00tAeotkfQvdx+tlQkBkL6xyvkyyvnWyvIx7qWR4HWV47GW6HFeZua73EV0KJr8Q6yFaYNWrWYX1nrTX+iJ/2UDvBSDAeoJ1haCoz/lyjafA3UNdRF/8joLajljp/bNlRR2uw1Wpdxx7RaHR/K1tH6Ah30BxsJhztOVCvN6Yy42XoPI/uMBw4TOvhNJ3xpd204zScOG1O3DY3LtOFy2Y93KY7vu61ezmv/LzDqqcjJWEshBDHmMNmUJThpijj0K1ugEhU09wRorEjREN7kKZ2a9kYWza0B2loC1HXFmBHbTsrdzXS0BYkHO17AFmayxYLZwfa7+f5vR+Q5bGTFQvwTM/+UM9w28nw2Elz7u9GV0rFB5klotBbSKG3MKF9+6K1piPc0eO+4U3BpniYdz0/33XZ2W0fjAQJRALxZSASoD3UTiAaIBAO4A/76Yh04A/7CUW792BkODMkjIUQ4mRlGsoKSa+DIRy65Q2x88uBMPWtQZo6Q7wjRGN7KB7indt3tWpWV1gB3uzvu+VpKEh3d7au7aTHlhluqws9w22Fe3rX57Gl224eddexUso6h233UETRUR0rEeFomEAkQEe493A+liSMhRAiBSilSHdZ55QPZdGiRcyaNQuAcCRKY0eIxvYgDe0h6tuslnhTR++PioaO+HrkIC1xu2mVJ8NtJ81tJ91liwd6ustOmstGustGmstOutsW6/bfv/Q5bId9bvxodY5I99oT+wMoqZ993D9RCCHEgGEzjdhob+dhva9zEFtn67uzWz3+3G9ta/Zb+zR3hKhs7KA5tk8ocvBrspUCn9Nm/YHh7gxvK7itPzq6BHeX9a7bk9E6P14kjIUQQhy2roPYSrMO771aawLhKM3+EC3+cOwR6rbsDHIr1K1lZWMH6/eG4u87FJuh4l3r6S6b1SqPP48FudOG12nD5zTxxtf3b89wH7qnIRkkjIUQQhxXSilcdhOX3SQ/7ciOEY1qWoNhWg8M80D3QG86sHXe0EGzP7HWuc9p48MfnX9kBTxMEsZCCCFOOIaR+Dny3nS2zlsDYdoC4dgyEl9vDYSJ9nV92TEgYSyEEOKk07V1frjny48FmZBUCCGE6GcSxkIIIUQ/kzAWQggh+pmEsRBCCNHPJIyFEEKIfiZhLIQQQvQzCWMhhBCin0kYCyGEEP1MwlgIIYToZxLGQgghRD+TMBZCCCH6mYSxEEII0c8kjIUQQoh+JmEshBBC9DMJYyGEEKKfSRgLIYQQ/UzCWAghhOhnCYWxUmqOUmqjUmqLUur2Xl53KqX+Hnv9XaXU4KSXVAghhEhRhwxjpZQJPABcAIwFrlJKjT1gty8CDVrr4cCvgJ8lu6BCCCFEqkqkZTwN2KK13qa1DgJPAXMP2Gcu8Hhs/VngXKWUSl4xhRBCiNSVSBiXALu7PK+Ibet1H611GGgCcpJRQCGEECLV2Y7nhymlbgJuij1tVUptTOLhc4HaJB7vZCZ1mTxSl8kjdZk8UpfJcbj1WN7XC4mEcSVQ1uV5aWxbb/tUKKVsQAZQd+CBtNbzgfkJfOZhU0qt0FpPPRbHPtlIXSaP1GXySF0mj9RlciSzHhPppl4OjFBKDVFKOYDPAi8dsM9LwLWx9c8Ab2mtdTIKKIQQQqS6Q7aMtdZhpdRXgdcAE3hEa/2RUurHwAqt9UvAn4AnlVJbgHqswBZCCCFEAhI6Z6y1XgAsOGDbD7qs+4HLk1u0w3ZMur9PUlKXySN1mTxSl8kjdZkcSatHJb3JQgghRP+S6TCFEEKIfpYSYXyo6TpF35RSjyilqpVSH3bZlq2Uel0ptTm2zOrPMp4IlFJlSqmFSql1SqmPlFJfj22XujxMSimXUuo9pdTqWF3+KLZ9SGy63S2x6Xcd/V3WE4VSylRKfaCUejn2XOryCCildiil1iqlVimlVsS2JeVn/IQP4wSn6xR9ewyYc8C224E3tdYjgDdjz8XBhYFvaK3HAtOBr8T+H0pdHr4AMFtrPRGYBMxRSk3Hmmb3V7FpdxuwpuEVifk6sL7Lc6nLI3eO1npSl0uakvIzfsKHMYlN1yn6oLVejDUCvquu05s+Dlx6PMt0ItJa79Var4ytt2D94itB6vKwaUtr7Kk99tDAbKzpdkHqMmFKqVLgk8AfY88VUpfJlJSf8VQI40Sm6xSHp0BrvTe2vg8o6M/CnGhidy2bDLyL1OURiXWrrgKqgdeBrUBjbLpdkJ/zw/Fr4NtANPY8B6nLI6WBfyul3o/NKAlJ+hk/rtNhihOP1lorpWTIfYKUUj7gOeB/tNbNXe+XInWZOK11BJiklMoEXgBG92+JTkxKqYuAaq31+0qpWf1cnFRwpta6UimVD7yulNrQ9cWj+RlPhZZxItN1isNTpZQqAogtq/u5PCcEpZQdK4j/orV+PrZZ6vIoaK0bgYXADCAzNt0uyM95oj4GXKKU2oF1Cm828BukLo+I1roytqzG+iNxGkn6GU+FME5kuk5xeLpOb3ot8I9+LMsJIXYe7k/Aeq31L7u8JHV5mJRSebEWMUopN3Ae1jn4hVjT7YLUZUK01t/VWpdqrQdj/W58S2t9NVKXh00p5VVKpXWuA58APiRJP+MpMemHUupCrPMindN13tO/JTpxKKX+BszCuvtIFfBD4EXgaWAQsBO4Qmt94CAv0YVS6kzgbWAt+8/N3YF13ljq8jAopSZgDYQxsRoMT2utf6yUGorVussGPgA+r7UO9F9JTyyxbupvaq0vkro8fLE6eyH21Ab8VWt9j1IqhyT8jKdEGAshhBAnslTophZCCCFOaBLGQgghRD+TMBZCCCH6mYSxEEII0c8kjIUQQoh+JmEshBBC9DMJYyGEEKKfSRgLIYQQ/ez/A8tF2ScuO7bZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9730: 0s - loss: 0.1232 - accura\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09027296304702759, 0.9729999899864197]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.32941177, 0.7254902 , 0.62352943, 0.5921569 ,\n",
       "         0.23529412, 0.14117648, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.87058824, 0.99607843, 0.99607843, 0.99607843,\n",
       "         0.99607843, 0.94509804, 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.6666667 , 0.20392157, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2627451 , 0.44705883, 0.28235295, 0.44705883,\n",
       "         0.6392157 , 0.8901961 , 0.99607843, 0.88235295, 0.99607843,\n",
       "         0.99607843, 0.99607843, 0.98039216, 0.8980392 , 0.99607843,\n",
       "         0.99607843, 0.54901963, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "         0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "         0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.3254902 , 0.99215686,\n",
       "         0.81960785, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.08627451, 0.9137255 , 1.        ,\n",
       "         0.3254902 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5058824 , 0.99607843, 0.93333334,\n",
       "         0.17254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.23137255, 0.9764706 , 0.99607843, 0.24313726,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.03529412, 0.8039216 , 0.972549  , 0.22745098, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.49411765, 0.99607843, 0.7137255 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29411766,\n",
       "         0.9843137 , 0.9411765 , 0.22352941, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.07450981, 0.8666667 ,\n",
       "         0.99607843, 0.6509804 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "         0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14901961, 0.99607843, 0.99607843,\n",
       "         0.3019608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.12156863, 0.8784314 , 0.99607843, 0.4509804 ,\n",
       "         0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.23921569, 0.9490196 , 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.8117647 , 0.07058824, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.6733916e-06, 2.9396406e-07, 1.8251505e-04, 1.5268563e-03,\n",
       "        2.9056949e-07, 5.8489313e-06, 7.3265255e-10, 9.9824476e-01,\n",
       "        1.6131806e-05, 1.7545648e-05]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test[:1])\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[2].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362.8125"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11610/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.8455 - val_loss: 0.5260\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8631 - val_loss: 0.4763\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4831 - val_loss: 0.4512\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4633 - val_loss: 0.4374\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4567 - val_loss: 0.4327\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4208\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4488 - val_loss: 0.4219\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4443 - val_loss: 0.4189\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4082\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4204 - val_loss: 0.4022\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4262 - val_loss: 0.4019\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4232 - val_loss: 1.6190\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4298 - val_loss: 0.3986\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.3872\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.3893\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.8669\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4092 - val_loss: 0.3867\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.3768\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3801\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3976 - val_loss: 0.3708\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu',\n",
    "                      input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "             optimizer = \"sgd\")\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs = 20,\n",
    "                   validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*30 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 30)                270       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3747\n",
      "0.3746512234210968\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8314105],\n",
       "       [1.8468995],\n",
       "       [1.4810897],\n",
       "       [2.6112893],\n",
       "       [1.9384623]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test[:5])\n",
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3915\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3951\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3894\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.3823\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3802\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.3828\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 947us/step - loss: 0.4044\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3748\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3728\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3705\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3873\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3783\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 941us/step - loss: 0.3725\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.3671\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3713\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 941us/step - loss: 0.3619\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 932us/step - loss: 0.3600\n",
      "Epoch 18/30\n",
      "328/363 [==========================>...] - ETA: 0s - loss: 0.3549"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MIGUEL~1\\AppData\\Local\\Temp/ipykernel_8964/577377571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                    \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                    callbacks = [checkpoint_cb])\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[0;32m   1210\u001b[0m                 \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1248\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1252\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    647\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msparse_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4062\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4063\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 4064\u001b[1;33m         _ctx, \"Identity\", name, input)\n\u001b[0m\u001b[0;32m   4065\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4066\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=30,\n",
    "                   callbacks = [checkpoint_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.3297\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.3415\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3313 - val_loss: 0.3280\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3326 - val_loss: 0.3424\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3308 - val_loss: 0.3260\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3306 - val_loss: 0.3255\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 0.3230\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3276 - val_loss: 0.3265\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3270 - val_loss: 0.3251\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3266 - val_loss: 0.3222\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.4284\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3248\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3277 - val_loss: 0.3193\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.3217\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3822\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3292 - val_loss: 0.3222\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.3239\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3222 - val_loss: 0.3202\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=50,\n",
    "                   validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [early_stopping_cb, checkpoint_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
